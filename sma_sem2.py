# -*- coding: utf-8 -*-
"""SMA Sem2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGJvZu7kXxEWJxq4mGyBcrv-NzddqsIn

# Prac 4
Develop Content (text, emoticons, image, audio, video) based social media analytics model for business

https://drive.google.com/file/d/1pNSAgVsWkBjPFOOIYx8RNDEHvk2YU0kp/view
"""

#Importing necessary libraries
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import re
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
#Import NLTK package
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
#Import Dataset
data = pd.read_csv('flipkart_data.csv')
data.head()

# unique ratings
pd.unique(data['rating'])
#plot the arrays
sns.countplot(data=data, x='rating', order=data.rating.value_counts().index)

# rating label(final)
pos_neg = []
for i in range(len(data['rating'])):
  if data['rating'][i] >= 5:
    pos_neg.append(1)
  else:
    pos_neg.append(0)
data['label'] = pos_neg
from tqdm import tqdm
def preprocess_text(text_data):
  preprocessed_text = []
  for sentence in tqdm(text_data):
    # Removing punctuations
    sentence = re.sub(r'[^\w\s]', '', sentence)
    # Converting lowercase and removing stopwords
    preprocessed_text.append(' '.join(token.lower()
    for token in nltk.word_tokenize(sentence)
      if token.lower() not in stopwords.words('english')))
  return preprocessed_text

import nltk
nltk.download('punkt_tab')
preprocessed_review = preprocess_text(data['review'].values)
data['review'] = preprocessed_review

data.head()

#Check Positive / Negative sentiments
data["label"].value_counts()

#Word cloud of Positive label = 1
consolidated = ' '.join( word for word in data['review'][data['label'] == 1].astype(str))

wordCloud = WordCloud(width=1600, height=800, random_state=None, max_font_size=110)

plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

#Word cloud of Negative label = 0
consolidated = ' '.join( word for word in data['review'][data['label'] == 0].astype(str))

wordCloud = WordCloud(width=1600, height=800, random_state=None, max_font_size=110)

plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Prac 8
: Use Graph Neural Networks on the datasets
"""

!python -c "import torch; print(torch.version.cuda);"
!pip install -q torch-scatter==latest+cu101 -f https://pytorchgeometric.com/whl/torch-1.6.0.html
!pip install -q torch-sparse==latest+cu101 -f https://pytorchgeometric.com/whl/torch-1.6.0.html
!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git

from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures
dataset = Planetoid(root='data/Planetoid', name='Cora',
transform=NormalizeFeatures())
print(f'Dataset: {dataset}:')
print('======================')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')
data = dataset[0] # Get the first graph object.
print(data)

print(data.x.shape)
data.x[0][:50]

import torch
from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
  def __init__(self, hidden_channels):
    super(GCN, self).__init__()
    torch.manual_seed(42)

    # Initialize the layers
    self.conv1 = GCNConv(dataset.num_features, hidden_channels)
    self.conv2 = GCNConv(hidden_channels, hidden_channels)
    self.out = Linear(hidden_channels, dataset.num_classes)

  def forward(self, x, edge_index):
    # First Message Passing Layer (Transformation)
    x = self.conv1(x, edge_index)
    x = x.relu()
    x = F.dropout(x, p=0.5, training=self.training)

    # Second Message Passing Layer
    x = self.conv2(x, edge_index)
    x = x.relu()
    x = F.dropout(x, p=0.5, training=self.training)

    # Output layer
    x = F.softmax(self.out(x), dim=1)
    return x

model = GCN(hidden_channels=16)
print(model)

# Initialize Optimizer
learning_rate = 0.01
decay = 5e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)

criterion = torch.nn.CrossEntropyLoss()

def train():
  model.train()
  optimizer.zero_grad()
  out = model(data.x, data.edge_index)
  loss = criterion(out[data.train_mask], data.y[data.train_mask])
  loss.backward()
  optimizer.step()
  return loss

def test():
  model.eval()
  out = model(data.x, data.edge_index)
  # Use the class with highest probability.
  pred = out.argmax(dim=1)

  # Check against ground-truth labels.
  test_correct = pred[data.test_mask] == data.y[data.test_mask]

  # Derive ratio of correct predictions.
  test_acc = int(test_correct.sum()) / int(data.test_mask.sum())
  return test_acc

losses = []
for epoch in range(0, 1001):
  loss = train()
  losses.append(loss)
  if epoch % 100 == 0:
    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')

import seaborn as sns

losses_float = [float(loss.cpu().detach().numpy()) for loss in losses]
loss_indices = [i for i,l in enumerate(losses_float)]
plt = sns.lineplot(x=loss_indices ,y=losses_float)
plt

"""# Prac 9
Analyze Twitter conversations to identify the most active and influential users using
Machine Learning Algorithms with Gephi Tool.

https://www.kaggle.com/datasets/eliasdabbas/5000-justdoit-tweets-dataset
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install advertools
# %config InlineBackend.figure_format = 'retina'
import matplotlib.pyplot as plt
import pandas as pd
import advertools as adv
import networkx as nx
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 280)
adv.__version__

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 280)
tweets_users_df = pd.read_csv('Tweets.csv', )
print(tweets_users_df.shape)
tweets_users_df.head(3)

[x for x in dir(adv) if x.startswith('extract')] # currently available extract functions
hashtag_summary = adv.extract_hashtags(tweets_users_df['tweet_full_text'])
hashtag_summary.keys()
hashtag_summary['overview']
hashtag_summary['hashtags'][:10]
hashtag_summary['hashtags_flat'][:10]
hashtag_summary['hashtag_counts'][:20]
hashtag_summary['hashtag_freq'][:15]

plt.figure(facecolor='#ebebeb', figsize=(11, 8))
plt.bar([x[0] for x in hashtag_summary['hashtag_freq'][:15]],  [x[1] for x in hashtag_summary['hashtag_freq'][:15]])
plt.title('Hashtag frequency', fontsize=18)

plt.xlabel('Hashtags per tweet', fontsize=12)
plt.ylabel('Number of tweets', fontsize=12)
plt.xticks(range(16))
plt.yticks(range(0, 2100, 100))
plt.grid(alpha=0.5)
plt.gca().set_frame_on(False)
hashtag_summary['top_hashtags'][:10]
plt.figure(facecolor='#ebebeb', figsize=(8, 12))
plt.barh([x[0] for x in hashtag_summary['top_hashtags'][2:][:30]][::-1],  [x[1] for x in hashtag_summary['top_hashtags'][2:][:30]][::-1])
plt.title('Top Hashtags')
plt.grid(alpha=0.5)
plt.gca().set_frame_on(False)

mention_summary = adv.extract_mentions(tweets_users_df['tweet_full_text'])
mention_summary.keys()
mention_summary['overview']
mention_summary['mentions'][:15]

mention_summary['mentions_flat'][:10]
mention_summary['mention_counts'][:20]
mention_summary['mention_freq'][:15]
plt.figure(facecolor='#ebebeb', figsize=(8, 8))
plt.bar([x[0] for x in mention_summary['mention_freq'][:15]],  [x[1] for x in mention_summary['mention_freq'][:15]])
plt.title('Mention frequency', fontsize=18)
plt.xlabel('Mention per tweet', fontsize=12)
plt.ylabel('Number of tweets', fontsize=12)
plt.xticks(range(15))
plt.yticks(range(0, 2800, 200))
plt.grid(alpha=0.5)
plt.gca().set_frame_on(False)
mention_summary['top_mentions'][:10]
plt.figure(facecolor='#ebebeb', figsize=(8, 8))
plt.barh([x[0] for x in mention_summary['top_mentions'][:15]][::-1],  [x[1] for x in mention_summary['top_mentions'][:15]][::-1])
plt.title('Top Mentions')
plt.grid(alpha=0.5)
plt.xticks(range(0, 1100, 100))
plt.gca().set_frame_on(False)

pip install advertools networkx matplotlib pandas
# Creating Knowledge Graph
!pip install keybert networkx TextNet
!pip install keybert networkx nltk
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import advertools as adv
tweets_users_df = pd.read_csv('Tweets.csv')
hashtag_summary = adv.extract_hashtags(tweets_users_df['tweet_full_text'])
mention_summary = adv.extract_mentions(tweets_users_df['tweet_full_text'])
G = nx.DiGraph()

for hashtags in hashtag_summary['hashtags']:
  for i in range(len(hashtags)):
    for j in range(i + 1, len(hashtags)):
      if G.has_edge(hashtags[i], hashtags[j]):
       G[hashtags[i]][hashtags[j]]['weight'] += 1
      else:
        G.add_edge(hashtags[i], hashtags[j], weight=1)

for mentions in mention_summary['mentions']:
  for i in range(len(mentions)):
    for j in range(i + 1, len(mentions)):
      if G.has_edge(mentions[i], mentions[j]):
        G[mentions[i]][mentions[j]]['weight'] += 1
      else:
        G.add_edge(mentions[i], mentions[j], weight=1)

plt.figure(figsize=(15, 15))
pos = nx.spring_layout(G, k=0.1)
nx.draw(G, pos, with_labels=True, node_size=50, font_size=10, edge_color='#BBBBBB')
plt.title('Knowledge Graph of Tweets', fontsize=20)
plt.show()

